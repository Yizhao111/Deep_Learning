{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import heapq\n",
    "import time\n",
    "import random\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import rankdata\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from matplotlib import rc\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "from scipy.integrate import trapz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concrete Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we used is the \"Californai Housing Prices\", the link is https://www.kaggle.com/camnugent/california-housing-prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generating(traindf, testdf):\n",
    "    # training set\n",
    "    trainx = traindf.drop('median_house_value', axis=1).values\n",
    "    trainy = traindf['median_house_value'].values\n",
    "    # test set\n",
    "    testx = testdf.drop('median_house_value', axis=1).values\n",
    "    testy = testdf['median_house_value'].values\n",
    "   \n",
    "    scalerx = preprocessing.MinMaxScaler().fit(trainx)\n",
    "    trainx = scalerx.transform(trainx) + 0.5\n",
    "    testx = scalerx.transform(testx) + 0.5\n",
    "\n",
    "    scalery = preprocessing.MinMaxScaler().fit(trainy.reshape(-1,1))\n",
    "    trainy = scalery.transform(trainy.reshape(-1,1)).reshape(1, -1)\n",
    "    testy = scalery.transform(testy.reshape(-1,1)).reshape(1, -1)\n",
    "      \n",
    "    \n",
    "    # transfer data into tensor\n",
    "    x = torch.tensor(trainx, dtype = torch.float)\n",
    "    y = torch.tensor(trainy[0], dtype = torch.float)\n",
    "    x_test = torch.tensor(testx, dtype = torch.float)\n",
    "    y_test = torch.tensor(testy[0], dtype = torch.float)\n",
    "    \n",
    "    return x, y, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteDropout(nn.Module):\n",
    "    def __init__(self, size = 1, weight_regularizer=1e-6,\n",
    "                 dropout_regularizer=1e-5, init_min=0.1, init_max=0.9):\n",
    "        \n",
    "        super(ConcreteDropout, self).__init__()\n",
    "        \n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "        \n",
    "        init_min = np.log(init_min) - np.log(1. - init_min)\n",
    "        init_max = np.log(init_max) - np.log(1. - init_max)\n",
    "        \n",
    "        self.p_logit = nn.Parameter(torch.empty(size).uniform_(init_min, init_max))\n",
    "        \n",
    "    def forward(self, x, layer):\n",
    "        p = torch.sigmoid(self.p_logit)\n",
    "            \n",
    "        out = layer(self._concrete_dropout(x, p))\n",
    "\n",
    "        sum_of_square = 0\n",
    "        \n",
    "        network_weights = torch.sum(torch.sum(torch.pow(layer.weight, 2), 0) / (1 - p))\n",
    "        network_bias = torch.sum(torch.pow(layer.bias, 2))   \n",
    "            \n",
    "        weights_regularizer = self.weight_regularizer * (network_bias + network_weights)\n",
    "\n",
    "        dropout_regularizer = p * torch.log(p) + (1. - p) * torch.log(1. - p)\n",
    "        dropout_regularizer = self.dropout_regularizer * torch.sum(dropout_regularizer)\n",
    "\n",
    "        regularization = weights_regularizer + dropout_regularizer\n",
    "\n",
    "        return out, regularization\n",
    "        \n",
    "    def _concrete_dropout(self, x, p):\n",
    "        eps = 1e-7\n",
    "        temp = 0.1\n",
    "\n",
    "        unif_noise = torch.rand_like(x)\n",
    "\n",
    "        drop_prob = (torch.log(p + eps)\n",
    "                    - torch.log(1 - p + eps)\n",
    "                    + torch.log(unif_noise + eps)\n",
    "                    - torch.log(1 - unif_noise + eps))\n",
    "        \n",
    "        drop_prob = torch.sigmoid(drop_prob / temp)\n",
    "        random_tensor = 1 - drop_prob\n",
    "        retain_prob = 1 - p\n",
    "        \n",
    "        x  = torch.mul(x, random_tensor)\n",
    "        x /= retain_prob\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_var(nn.Module):\n",
    "    def __init__(self, weight_regularizer, dropout_regularizer):\n",
    "        super(Model_var, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 50)\n",
    "        self.linear2 = nn.Linear(50, 50)\n",
    "        self.linear3 = nn.Linear(50, 20)\n",
    "        \n",
    "        \n",
    "        self.linear4_mu = nn.Linear(20, 1)\n",
    "        self.linear4_var = nn.Linear(20, 1)\n",
    "        \n",
    "        self.conc_drop1 = ConcreteDropout(size = 8, weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop2 = ConcreteDropout(size = 50, weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop3 = ConcreteDropout(size = 50, weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        \n",
    "        self.conc_drop_mu = ConcreteDropout(size = 20, weight_regularizer=weight_regularizer,\n",
    "                                             dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop_logvar = ConcreteDropout(size = 20, weight_regularizer=weight_regularizer,\n",
    "                                                 dropout_regularizer=dropout_regularizer)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            regularization = torch.empty(5)\n",
    "\n",
    "            x1, regularization[0] = self.conc_drop1(x, self.linear1)\n",
    "            x1 = self.relu(x1)\n",
    "            x2, regularization[1] = self.conc_drop2(x1, self.linear2)\n",
    "            x2 = self.relu(x2)\n",
    "            x3, regularization[2] = self.conc_drop3(x2, self.linear3)\n",
    "            x3 = self.relu(x3)\n",
    "            output, regularization[3] = self.conc_drop_mu(x3, self.linear4_mu)\n",
    "            log_var, regularization[4] = self.conc_drop_mu(x3, self.linear4_var)\n",
    "\n",
    "            return output, log_var, regularization.sum()\n",
    "        else:\n",
    "            x = self.relu(self.linear1(x))\n",
    "            x = self.relu(self.linear2(x))\n",
    "            x = self.relu(self.linear3(x))\n",
    "            x = self.linear4_mu(x)\n",
    "            return x\n",
    "\n",
    "def heteroscedastic_loss_var(true, mean, log_var):\n",
    "    precision = torch.exp(-log_var)\n",
    "    return torch.mean(torch.sum(precision * (true - mean)**2 + log_var, 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main_effect(nn.Module):\n",
    "    def __init__(self, num_dim):\n",
    "        super(Main_effect, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance, patience):\n",
    "        \n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "    \n",
    "    def stop_criterion(self, val_errors):\n",
    "        if len(val_errors) < self.patience + 1:\n",
    "            return False\n",
    "        else:\n",
    "            current_best = min(val_errors[:-self.patience])\n",
    "            current_stop = True\n",
    "            for i in range(self.patience):\n",
    "                current_stop = current_stop and (val_errors[-i-1] - current_best > self.tolerance)\n",
    "            return current_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_CD_var(mlp, main_effect, x, y, x_test, y_test, learning_rate = 0.001, batch_size = 50, num_epoch=1000, tolerance=0.002, patience = 20):\n",
    "    \n",
    "    parameters = set(main_effect.parameters()) | set(mlp.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr = learning_rate)\n",
    "    early_stop = EarlyStopping(tolerance, patience)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    num_data, num_dim = x.shape\n",
    "    y = y.view(-1, 1)\n",
    "    data = torch.cat((x, y), 1)\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        # permuate the data\n",
    "        data_perm = data[torch.randperm(len(data))]\n",
    "        x = data_perm[:, 0:-1]\n",
    "        y = data_perm[:, -1]\n",
    "\n",
    "        for index in range(int(num_data/batch_size)):\n",
    "            # data comes in\n",
    "            inputs = x[index*batch_size : (index+1)*batch_size]\n",
    "            labels = y[index*batch_size : (index+1)*batch_size].view(-1,1)\n",
    "\n",
    "            # initialize the gradient of optimizer\n",
    "            optimizer.zero_grad()\n",
    "            mlp.train()\n",
    "            # calculate the loss function\n",
    "\n",
    "            output_mlp, var, reg = mlp(inputs)          \n",
    "            loss = heteroscedastic_loss_var(labels, output_mlp + main_effect(inputs), var) + reg\n",
    "\n",
    "            # backpropogate the gradient     \n",
    "            loss.backward()\n",
    "            # optimize with SGD\n",
    "            optimizer.step()\n",
    "\n",
    "        # train and validation loss\n",
    "        mlp.eval()\n",
    "        train_errors.append(criterion(mlp.forward(x) + main_effect.forward(x), y.view(-1,1)))\n",
    "        val_errors.append(criterion(mlp.forward(x_test) + main_effect.forward(x_test), y_test.view(-1,1)))\n",
    "\n",
    "        # determine if early stop\n",
    "        if early_stop.stop_criterion(val_errors):\n",
    "            print(val_errors[epoch])\n",
    "            print('Stop after %d epochs' % epoch)\n",
    "            break\n",
    "\n",
    "        if (epoch % 10) == 0:\n",
    "            print('EPOACH %d: TRAIN LOSS: %.4f; VAL LOSS IS: %.5f.'% (epoch+1, train_errors[epoch], val_errors[epoch]))\n",
    "        torch.save(mlp.state_dict(), 'CD_mlp_01.pth')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove none value\n",
    "df = pd.read_csv(\"housing.csv\", sep=\",\")\n",
    "df = df.dropna(axis=0)\n",
    "# remove ocean_proximity\n",
    "df = df.drop('ocean_proximity', axis = 1)\n",
    "# split data into training and test set\n",
    "\n",
    "np.random.seed(129)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "traindf = df[msk]\n",
    "testdf = df[~msk]\n",
    "x, y, x_test, y_test = data_generating(traindf, testdf)\n",
    "num_data, num_feature = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1e-4\n",
    "wr = l**2. / num_data\n",
    "dr = 2. / num_data\n",
    "learning_rate = 0.001\n",
    "batch_size = 50; num_epoch = 1000;\n",
    "tolerance = 0.01; patience = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_cd_var = Model_var(wr, dr)\n",
    "main_effect_cd_var = Main_effect(num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOACH 1: TRAIN LOSS: 0.0444; VAL LOSS IS: 0.04494.\n",
      "EPOACH 11: TRAIN LOSS: 0.0239; VAL LOSS IS: 0.02453.\n",
      "EPOACH 21: TRAIN LOSS: 0.0230; VAL LOSS IS: 0.02377.\n",
      "EPOACH 31: TRAIN LOSS: 0.0231; VAL LOSS IS: 0.02399.\n",
      "EPOACH 41: TRAIN LOSS: 0.0223; VAL LOSS IS: 0.02313.\n",
      "EPOACH 51: TRAIN LOSS: 0.0220; VAL LOSS IS: 0.02290.\n",
      "EPOACH 61: TRAIN LOSS: 0.0218; VAL LOSS IS: 0.02275.\n",
      "EPOACH 71: TRAIN LOSS: 0.0216; VAL LOSS IS: 0.02251.\n",
      "EPOACH 81: TRAIN LOSS: 0.0213; VAL LOSS IS: 0.02219.\n",
      "EPOACH 91: TRAIN LOSS: 0.0216; VAL LOSS IS: 0.02258.\n",
      "EPOACH 101: TRAIN LOSS: 0.0218; VAL LOSS IS: 0.02262.\n",
      "EPOACH 111: TRAIN LOSS: 0.0208; VAL LOSS IS: 0.02170.\n",
      "EPOACH 121: TRAIN LOSS: 0.0206; VAL LOSS IS: 0.02141.\n",
      "EPOACH 131: TRAIN LOSS: 0.0206; VAL LOSS IS: 0.02150.\n",
      "EPOACH 141: TRAIN LOSS: 0.0201; VAL LOSS IS: 0.02091.\n",
      "EPOACH 151: TRAIN LOSS: 0.0195; VAL LOSS IS: 0.02021.\n",
      "EPOACH 161: TRAIN LOSS: 0.0185; VAL LOSS IS: 0.01940.\n",
      "EPOACH 171: TRAIN LOSS: 0.0191; VAL LOSS IS: 0.01976.\n",
      "EPOACH 181: TRAIN LOSS: 0.0189; VAL LOSS IS: 0.01961.\n",
      "EPOACH 191: TRAIN LOSS: 0.0183; VAL LOSS IS: 0.01902.\n",
      "EPOACH 201: TRAIN LOSS: 0.0185; VAL LOSS IS: 0.01916.\n",
      "EPOACH 211: TRAIN LOSS: 0.0181; VAL LOSS IS: 0.01890.\n",
      "EPOACH 221: TRAIN LOSS: 0.0180; VAL LOSS IS: 0.01889.\n",
      "EPOACH 231: TRAIN LOSS: 0.0183; VAL LOSS IS: 0.01909.\n",
      "EPOACH 241: TRAIN LOSS: 0.0183; VAL LOSS IS: 0.01916.\n",
      "EPOACH 251: TRAIN LOSS: 0.0180; VAL LOSS IS: 0.01888.\n",
      "EPOACH 261: TRAIN LOSS: 0.0181; VAL LOSS IS: 0.01898.\n",
      "EPOACH 271: TRAIN LOSS: 0.0178; VAL LOSS IS: 0.01872.\n",
      "EPOACH 281: TRAIN LOSS: 0.0182; VAL LOSS IS: 0.01914.\n",
      "EPOACH 291: TRAIN LOSS: 0.0180; VAL LOSS IS: 0.01901.\n",
      "EPOACH 301: TRAIN LOSS: 0.0192; VAL LOSS IS: 0.02005.\n",
      "EPOACH 311: TRAIN LOSS: 0.0181; VAL LOSS IS: 0.01902.\n",
      "EPOACH 321: TRAIN LOSS: 0.0179; VAL LOSS IS: 0.01896.\n",
      "EPOACH 331: TRAIN LOSS: 0.0178; VAL LOSS IS: 0.01879.\n",
      "EPOACH 341: TRAIN LOSS: 0.0205; VAL LOSS IS: 0.02151.\n",
      "EPOACH 351: TRAIN LOSS: 0.0168; VAL LOSS IS: 0.01776.\n",
      "EPOACH 361: TRAIN LOSS: 0.0164; VAL LOSS IS: 0.01732.\n",
      "EPOACH 371: TRAIN LOSS: 0.0165; VAL LOSS IS: 0.01748.\n",
      "EPOACH 381: TRAIN LOSS: 0.0167; VAL LOSS IS: 0.01764.\n",
      "EPOACH 391: TRAIN LOSS: 0.0162; VAL LOSS IS: 0.01712.\n",
      "EPOACH 401: TRAIN LOSS: 0.0175; VAL LOSS IS: 0.01837.\n",
      "EPOACH 411: TRAIN LOSS: 0.0175; VAL LOSS IS: 0.01862.\n",
      "EPOACH 421: TRAIN LOSS: 0.0160; VAL LOSS IS: 0.01712.\n",
      "EPOACH 431: TRAIN LOSS: 0.0165; VAL LOSS IS: 0.01766.\n",
      "EPOACH 441: TRAIN LOSS: 0.0151; VAL LOSS IS: 0.01608.\n",
      "EPOACH 451: TRAIN LOSS: 0.0174; VAL LOSS IS: 0.01834.\n",
      "EPOACH 461: TRAIN LOSS: 0.0141; VAL LOSS IS: 0.01515.\n",
      "EPOACH 471: TRAIN LOSS: 0.0135; VAL LOSS IS: 0.01465.\n",
      "EPOACH 481: TRAIN LOSS: 0.0145; VAL LOSS IS: 0.01556.\n",
      "EPOACH 491: TRAIN LOSS: 0.0137; VAL LOSS IS: 0.01499.\n",
      "EPOACH 501: TRAIN LOSS: 0.0148; VAL LOSS IS: 0.01582.\n",
      "EPOACH 511: TRAIN LOSS: 0.0131; VAL LOSS IS: 0.01411.\n",
      "EPOACH 521: TRAIN LOSS: 0.0146; VAL LOSS IS: 0.01567.\n",
      "EPOACH 531: TRAIN LOSS: 0.0133; VAL LOSS IS: 0.01460.\n",
      "EPOACH 541: TRAIN LOSS: 0.0131; VAL LOSS IS: 0.01425.\n",
      "EPOACH 551: TRAIN LOSS: 0.0128; VAL LOSS IS: 0.01408.\n",
      "EPOACH 561: TRAIN LOSS: 0.0128; VAL LOSS IS: 0.01399.\n",
      "EPOACH 571: TRAIN LOSS: 0.0134; VAL LOSS IS: 0.01478.\n",
      "EPOACH 581: TRAIN LOSS: 0.0141; VAL LOSS IS: 0.01566.\n",
      "EPOACH 591: TRAIN LOSS: 0.0131; VAL LOSS IS: 0.01481.\n",
      "EPOACH 601: TRAIN LOSS: 0.0128; VAL LOSS IS: 0.01442.\n",
      "EPOACH 611: TRAIN LOSS: 0.0127; VAL LOSS IS: 0.01431.\n",
      "EPOACH 621: TRAIN LOSS: 0.0128; VAL LOSS IS: 0.01443.\n",
      "EPOACH 631: TRAIN LOSS: 0.0134; VAL LOSS IS: 0.01483.\n",
      "EPOACH 641: TRAIN LOSS: 0.0135; VAL LOSS IS: 0.01512.\n",
      "EPOACH 651: TRAIN LOSS: 0.0148; VAL LOSS IS: 0.01583.\n",
      "EPOACH 661: TRAIN LOSS: 0.0130; VAL LOSS IS: 0.01444.\n",
      "EPOACH 671: TRAIN LOSS: 0.0122; VAL LOSS IS: 0.01354.\n",
      "EPOACH 681: TRAIN LOSS: 0.0142; VAL LOSS IS: 0.01551.\n",
      "EPOACH 691: TRAIN LOSS: 0.0120; VAL LOSS IS: 0.01340.\n",
      "EPOACH 701: TRAIN LOSS: 0.0122; VAL LOSS IS: 0.01354.\n",
      "EPOACH 711: TRAIN LOSS: 0.0123; VAL LOSS IS: 0.01376.\n",
      "EPOACH 721: TRAIN LOSS: 0.0118; VAL LOSS IS: 0.01330.\n",
      "EPOACH 731: TRAIN LOSS: 0.0121; VAL LOSS IS: 0.01362.\n",
      "EPOACH 741: TRAIN LOSS: 0.0122; VAL LOSS IS: 0.01367.\n",
      "EPOACH 751: TRAIN LOSS: 0.0139; VAL LOSS IS: 0.01482.\n",
      "EPOACH 761: TRAIN LOSS: 0.0119; VAL LOSS IS: 0.01340.\n",
      "EPOACH 771: TRAIN LOSS: 0.0120; VAL LOSS IS: 0.01339.\n",
      "EPOACH 781: TRAIN LOSS: 0.0123; VAL LOSS IS: 0.01386.\n",
      "EPOACH 791: TRAIN LOSS: 0.0119; VAL LOSS IS: 0.01351.\n",
      "EPOACH 801: TRAIN LOSS: 0.0116; VAL LOSS IS: 0.01332.\n",
      "EPOACH 811: TRAIN LOSS: 0.0125; VAL LOSS IS: 0.01384.\n",
      "EPOACH 821: TRAIN LOSS: 0.0117; VAL LOSS IS: 0.01331.\n",
      "EPOACH 831: TRAIN LOSS: 0.0118; VAL LOSS IS: 0.01342.\n",
      "EPOACH 841: TRAIN LOSS: 0.0140; VAL LOSS IS: 0.01533.\n",
      "EPOACH 851: TRAIN LOSS: 0.0114; VAL LOSS IS: 0.01301.\n",
      "EPOACH 861: TRAIN LOSS: 0.0151; VAL LOSS IS: 0.01621.\n",
      "EPOACH 871: TRAIN LOSS: 0.0115; VAL LOSS IS: 0.01313.\n",
      "EPOACH 881: TRAIN LOSS: 0.0120; VAL LOSS IS: 0.01363.\n",
      "EPOACH 891: TRAIN LOSS: 0.0119; VAL LOSS IS: 0.01344.\n",
      "EPOACH 901: TRAIN LOSS: 0.0118; VAL LOSS IS: 0.01346.\n",
      "EPOACH 911: TRAIN LOSS: 0.0132; VAL LOSS IS: 0.01463.\n",
      "EPOACH 921: TRAIN LOSS: 0.0115; VAL LOSS IS: 0.01314.\n",
      "EPOACH 931: TRAIN LOSS: 0.0122; VAL LOSS IS: 0.01360.\n",
      "EPOACH 941: TRAIN LOSS: 0.0117; VAL LOSS IS: 0.01329.\n",
      "EPOACH 951: TRAIN LOSS: 0.0190; VAL LOSS IS: 0.02002.\n",
      "EPOACH 961: TRAIN LOSS: 0.0117; VAL LOSS IS: 0.01322.\n",
      "EPOACH 971: TRAIN LOSS: 0.0115; VAL LOSS IS: 0.01330.\n",
      "EPOACH 981: TRAIN LOSS: 0.0123; VAL LOSS IS: 0.01399.\n",
      "EPOACH 991: TRAIN LOSS: 0.0113; VAL LOSS IS: 0.01298.\n"
     ]
    }
   ],
   "source": [
    "#if skip_training:\n",
    "    \n",
    "training_CD_var(mlp_cd_var, main_effect_cd_var, x, y, x_test, y_test, learning_rate, batch_size, num_epoch, tolerance, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC sample \n",
    "K_test = 20 # sample 20 times \n",
    "mlp_cd_var.train() \n",
    "MC_samples = [mlp_cd_var(x_test)[0] + main_effect_cd_var(x_test) for _ in range(K_test)]  \n",
    "mean_samples = torch.stack([tup for tup in MC_samples]).view(K_test, x_test.shape[0]).cpu().data.numpy()  # shape (20, 4000) \n",
    "MC_sample_logvar = [mlp_cd_var(x_test)[1] for _ in range(K_test)]\n",
    "logvar = torch.stack([tup for tup in MC_sample_logvar]).view(K_test, x_test.shape[0]).cpu().data.numpy() # shape (20, 4000)\n",
    "var = np.exp(logvar)\n",
    "var = np.mean(var,0)\n",
    "\n",
    "for samples in mean_samples:\n",
    "    for i in range(len(samples)):\n",
    "        samples[i] = np.random.normal(samples[i], np.sqrt(var[i]))\n",
    "\n",
    "mean = np.mean(mean_samples, 0) # 4000*1\n",
    "concrete_uncertainty = np.std(mean_samples, 0)  # 4000*1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the area between two lines is =  -0.017057323293660498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5aa45e38d0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hit_probability(y_test, mean, concrete_uncertainty, confidence):\n",
    "    confidence_lower, confidence_higher = scipy.stats.norm.interval(confidence , mean, concrete_uncertainty)\n",
    "    return np.sum([1 if y_test[i] <= confidence_higher[i] and y_test[i] >= confidence_lower[i] else 0 for i in range(len(y_test))]) / len(y_test)\n",
    "\n",
    "confidence = np.linspace(0, 1.0, 21, endpoint = True)\n",
    "\n",
    "#hit = hit_probability(means, y_test, confidence)\n",
    "plt.plot(confidence, confidence, '-*')\n",
    "hit_ratio = [hit_probability(y_test, mean, concrete_uncertainty, c) for c in confidence]\n",
    "\n",
    "area1 = trapz(confidence, dx = 0.05)\n",
    "area2 = trapz(hit_ratio, dx = 0.05)\n",
    "print(\"the area between two lines is = \", area1 - area2)\n",
    "\n",
    "plt.plot(confidence, hit_ratio )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
