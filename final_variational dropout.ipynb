{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import colors\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import rankdata\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "from scipy.integrate import trapz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generating(traindf, testdf):\n",
    "    # training set\n",
    "    trainx = traindf.drop('median_house_value', axis=1).values\n",
    "    trainy = traindf['median_house_value'].values\n",
    "    # test set\n",
    "    testx = testdf.drop('median_house_value', axis=1).values\n",
    "    testy = testdf['median_house_value'].values\n",
    "   \n",
    "    scalerx = preprocessing.MinMaxScaler().fit(trainx)\n",
    "    trainx = scalerx.transform(trainx) + 0.5\n",
    "    testx = scalerx.transform(testx) + 0.5\n",
    "\n",
    "    scalery = preprocessing.MinMaxScaler().fit(trainy.reshape(-1,1))\n",
    "    trainy = scalery.transform(trainy.reshape(-1,1)).reshape(1, -1)\n",
    "    testy = scalery.transform(testy.reshape(-1,1)).reshape(1, -1)\n",
    "      \n",
    "    \n",
    "    # transfer data into tensor\n",
    "    x = torch.tensor(trainx, dtype = torch.float)\n",
    "    y = torch.tensor(trainy[0], dtype = torch.float)\n",
    "    x_test = torch.tensor(testx, dtype = torch.float)\n",
    "    y_test = torch.tensor(testy[0], dtype = torch.float)\n",
    "    \n",
    "    return x, y, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove none value\n",
    "df = pd.read_csv(\"housing.csv\", sep=\",\")\n",
    "df = df.dropna(axis=0)\n",
    "# remove ocean_proximity\n",
    "df = df.drop('ocean_proximity', axis = 1)\n",
    "# split data into training and test set\n",
    "\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "traindf = df[msk]\n",
    "testdf = df[~msk]\n",
    "x, y, x_test, y_test = data_generating(traindf, testdf)\n",
    "num_data, num_feature = x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negativeKL(log_alpha):\n",
    "    c1 = 1.16145124\n",
    "    c2 = -1.50204118\n",
    "    c3 = 0.58629921\n",
    "    alpha = log_alpha.exp()\n",
    "    negativeKL = 0.5 * log_alpha + c1 * alpha + c2 * alpha**2 + c3 * alpha**3\n",
    "    \n",
    "    return negativeKL\n",
    "\n",
    "class MLP_VD(nn.Module):\n",
    "    def __init__(self, size, max_log_alpha = -0.5):\n",
    "        super(MLP_VD, self).__init__()\n",
    "        self.size = size\n",
    "        # set maximum log variance because we are using a wild approximation\n",
    "        self.max_log_alpha = max_log_alpha\n",
    "        # variational dropout for each node\n",
    "        self.log_alpha = nn.ParameterList([nn.Parameter((torch.ones(size[i]) * 0.5).log()) for i in range(len(size) - 1)])\n",
    "        # layer\n",
    "        self.layers = nn.ModuleList([nn.Linear(size[i], size[i + 1]) for i in range(len(size) - 1)])\n",
    "    \n",
    "    def KL(self):\n",
    "        KL = 0\n",
    "        for i in range(len(self.log_alpha)):\n",
    "            KL = KL - negativeKL(self.log_alpha[i]).sum() * self.size[i+1]\n",
    "        return KL\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "        \n",
    "            # set limit to all variance\n",
    "            for i in range(len(self.log_alpha)):\n",
    "                self.log_alpha[i].data = torch.clamp(self.log_alpha[i].data, max=self.max_log_alpha)\n",
    "            \n",
    "            # forward pass\n",
    "            for i, j in enumerate(self.layers):\n",
    "                # generate noise for each layer\n",
    "                epsilon = Variable(torch.randn(x.shape[0], self.size[i]))\n",
    "                r = torch.randn(1)\n",
    "                epsilon = 1 + self.log_alpha[i].exp() * epsilon\n",
    "                if i != len(self.size) - 2:\n",
    "                    x = F.relu(j(x * epsilon))\n",
    "                else:\n",
    "                    x = j(x * epsilon)\n",
    "            return x\n",
    "        else:\n",
    "            for i, j in enumerate(self.layers):\n",
    "                if i != len(self.size) - 2:\n",
    "                    x = F.relu(j(x))\n",
    "                else:\n",
    "                    x = j(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001;\n",
    "lambda_r1 = 0; lambda_r2 = 0;\n",
    "batch_size = 50; num_epoch = 1000;\n",
    "tolerance = 0.01; patience = 20;\n",
    "#layers = [8, 50, 30, 1]\n",
    "layers = [8,50,50,20,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def training_VD(mlp, x, y, x_test, y_test,learning_rate = 0.001, batch_size = 50, num_epoch=1000, tolerance=0.002, patience = 20):\n",
    "    # Initialize training parameters\n",
    "    parameters = set(mlp.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(parameters, lr = learning_rate)\n",
    "    early_stop = EarlyStopping(tolerance, patience)\n",
    "    \n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    model_KL = []\n",
    "    \n",
    "    num_data, num_dim = x.shape\n",
    "    y = y.view(-1, 1)\n",
    "    data = torch.cat((x, y), 1)\n",
    "    \n",
    "    # generate a random seed for training\n",
    "    for epoch in range(num_epoch):\n",
    "        # permuate the data\n",
    "        data_perm = data[torch.randperm(len(data))]\n",
    "        x = data_perm[:, 0:-1]\n",
    "        y = data_perm[:, -1]\n",
    "        mlp.train()\n",
    "        for index in range(int(num_data/batch_size)):\n",
    "            # data comes in\n",
    "            inputs = x[index*batch_size : (index+1)*batch_size]\n",
    "            labels = y[index*batch_size : (index+1)*batch_size].view(-1,1)\n",
    "            \n",
    "            # initialize the gradient of optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # calculate the loss function\n",
    "            outputs = mlp.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss = loss + 0.1 * mlp.KL() / (num_data)\n",
    "            \n",
    "            # backpropogate the gradient\n",
    "            loss.backward()\n",
    "            # optimize with SGD\n",
    "            optimizer.step()\n",
    "        \n",
    "        # do not use dropout during testing\n",
    "        mlp.eval()\n",
    "        \n",
    "        # train and validation loss\n",
    "        train_errors.append(criterion(mlp.forward(x), y.view(-1,1)))\n",
    "        val_errors.append(criterion(mlp.forward(x_test), y_test.view(-1,1)))\n",
    "        model_KL.append(mlp.KL())\n",
    "        \n",
    "        # determine if early stop\n",
    "        if early_stop.stop_criterion(val_errors):\n",
    "            print(val_errors[epoch])\n",
    "            print('Stop after %d epochs' % epoch)\n",
    "            break\n",
    "        \n",
    "        if (epoch % 10) == 0:\n",
    "            print('EPOACH %d: TRAIN LOSS: %.4f; VAL LOSS IS: %.5f; KL IS : %.5f'% (epoch+1, train_errors[epoch], val_errors[epoch], model_KL[epoch]))\n",
    "\n",
    "                # save model\n",
    "    torch.save(mlp.state_dict(), 'VD_mlp_01.pth')\n",
    "    \n",
    "    return mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance, patience):\n",
    "        \n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "    \n",
    "    def stop_criterion(self, val_errors):\n",
    "        if len(val_errors) < self.patience + 1:\n",
    "            return False\n",
    "        else:\n",
    "            current_best = min(val_errors[:-self.patience])\n",
    "            current_stop = True\n",
    "            for i in range(self.patience):\n",
    "                current_stop = current_stop and (val_errors[-i-1] - current_best > self.tolerance)\n",
    "            return current_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOACH 1: TRAIN LOSS: 0.0501; VAL LOSS IS: 0.05145; KL IS : 68.56005\n",
      "EPOACH 11: TRAIN LOSS: 0.0250; VAL LOSS IS: 0.02535; KL IS : 211.35593\n",
      "EPOACH 21: TRAIN LOSS: 0.0220; VAL LOSS IS: 0.02213; KL IS : 322.27902\n",
      "EPOACH 31: TRAIN LOSS: 0.0202; VAL LOSS IS: 0.02033; KL IS : 352.58453\n",
      "EPOACH 41: TRAIN LOSS: 0.0200; VAL LOSS IS: 0.02009; KL IS : 395.82385\n",
      "EPOACH 51: TRAIN LOSS: 0.0180; VAL LOSS IS: 0.01787; KL IS : 439.30515\n",
      "EPOACH 61: TRAIN LOSS: 0.0178; VAL LOSS IS: 0.01781; KL IS : 467.51761\n",
      "EPOACH 71: TRAIN LOSS: 0.0177; VAL LOSS IS: 0.01771; KL IS : 478.90176\n",
      "EPOACH 81: TRAIN LOSS: 0.0178; VAL LOSS IS: 0.01790; KL IS : 487.86783\n",
      "EPOACH 91: TRAIN LOSS: 0.0179; VAL LOSS IS: 0.01774; KL IS : 490.72577\n",
      "EPOACH 101: TRAIN LOSS: 0.0173; VAL LOSS IS: 0.01721; KL IS : 493.36331\n",
      "EPOACH 111: TRAIN LOSS: 0.0175; VAL LOSS IS: 0.01733; KL IS : 496.82394\n",
      "EPOACH 121: TRAIN LOSS: 0.0168; VAL LOSS IS: 0.01677; KL IS : 496.40417\n",
      "EPOACH 131: TRAIN LOSS: 0.0169; VAL LOSS IS: 0.01684; KL IS : 499.39270\n",
      "EPOACH 141: TRAIN LOSS: 0.0176; VAL LOSS IS: 0.01735; KL IS : 498.67725\n",
      "EPOACH 151: TRAIN LOSS: 0.0170; VAL LOSS IS: 0.01697; KL IS : 499.21869\n",
      "EPOACH 161: TRAIN LOSS: 0.0171; VAL LOSS IS: 0.01708; KL IS : 501.34952\n",
      "EPOACH 171: TRAIN LOSS: 0.0167; VAL LOSS IS: 0.01653; KL IS : 505.27225\n",
      "EPOACH 181: TRAIN LOSS: 0.0172; VAL LOSS IS: 0.01685; KL IS : 503.82211\n",
      "EPOACH 191: TRAIN LOSS: 0.0174; VAL LOSS IS: 0.01723; KL IS : 505.27203\n",
      "EPOACH 201: TRAIN LOSS: 0.0168; VAL LOSS IS: 0.01687; KL IS : 506.76645\n",
      "EPOACH 211: TRAIN LOSS: 0.0166; VAL LOSS IS: 0.01651; KL IS : 509.73581\n",
      "EPOACH 221: TRAIN LOSS: 0.0164; VAL LOSS IS: 0.01642; KL IS : 510.76639\n",
      "EPOACH 231: TRAIN LOSS: 0.0166; VAL LOSS IS: 0.01667; KL IS : 508.87326\n",
      "EPOACH 241: TRAIN LOSS: 0.0165; VAL LOSS IS: 0.01676; KL IS : 512.60413\n",
      "EPOACH 251: TRAIN LOSS: 0.0161; VAL LOSS IS: 0.01604; KL IS : 514.02789\n",
      "EPOACH 261: TRAIN LOSS: 0.0163; VAL LOSS IS: 0.01634; KL IS : 512.17657\n",
      "EPOACH 271: TRAIN LOSS: 0.0161; VAL LOSS IS: 0.01621; KL IS : 514.04578\n",
      "EPOACH 281: TRAIN LOSS: 0.0173; VAL LOSS IS: 0.01747; KL IS : 516.61737\n",
      "EPOACH 291: TRAIN LOSS: 0.0171; VAL LOSS IS: 0.01683; KL IS : 515.13208\n",
      "EPOACH 301: TRAIN LOSS: 0.0163; VAL LOSS IS: 0.01632; KL IS : 514.39020\n",
      "EPOACH 311: TRAIN LOSS: 0.0164; VAL LOSS IS: 0.01643; KL IS : 513.89990\n",
      "EPOACH 321: TRAIN LOSS: 0.0161; VAL LOSS IS: 0.01620; KL IS : 514.64886\n",
      "EPOACH 331: TRAIN LOSS: 0.0162; VAL LOSS IS: 0.01640; KL IS : 518.73169\n",
      "EPOACH 341: TRAIN LOSS: 0.0178; VAL LOSS IS: 0.01797; KL IS : 516.22540\n",
      "EPOACH 351: TRAIN LOSS: 0.0163; VAL LOSS IS: 0.01639; KL IS : 520.77020\n",
      "EPOACH 361: TRAIN LOSS: 0.0159; VAL LOSS IS: 0.01613; KL IS : 517.96704\n",
      "EPOACH 371: TRAIN LOSS: 0.0159; VAL LOSS IS: 0.01594; KL IS : 517.10345\n",
      "EPOACH 381: TRAIN LOSS: 0.0160; VAL LOSS IS: 0.01625; KL IS : 516.73370\n",
      "EPOACH 391: TRAIN LOSS: 0.0162; VAL LOSS IS: 0.01630; KL IS : 517.44440\n",
      "EPOACH 401: TRAIN LOSS: 0.0166; VAL LOSS IS: 0.01642; KL IS : 517.10138\n",
      "EPOACH 411: TRAIN LOSS: 0.0162; VAL LOSS IS: 0.01648; KL IS : 516.30090\n",
      "EPOACH 421: TRAIN LOSS: 0.0162; VAL LOSS IS: 0.01621; KL IS : 513.97137\n",
      "EPOACH 431: TRAIN LOSS: 0.0162; VAL LOSS IS: 0.01617; KL IS : 521.09387\n",
      "EPOACH 441: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01583; KL IS : 521.57196\n",
      "EPOACH 451: TRAIN LOSS: 0.0159; VAL LOSS IS: 0.01590; KL IS : 522.27081\n",
      "EPOACH 461: TRAIN LOSS: 0.0160; VAL LOSS IS: 0.01606; KL IS : 520.01074\n",
      "EPOACH 471: TRAIN LOSS: 0.0159; VAL LOSS IS: 0.01616; KL IS : 522.36139\n",
      "EPOACH 481: TRAIN LOSS: 0.0162; VAL LOSS IS: 0.01640; KL IS : 520.77502\n",
      "EPOACH 491: TRAIN LOSS: 0.0159; VAL LOSS IS: 0.01609; KL IS : 521.07092\n",
      "EPOACH 501: TRAIN LOSS: 0.0160; VAL LOSS IS: 0.01617; KL IS : 520.59039\n",
      "EPOACH 511: TRAIN LOSS: 0.0163; VAL LOSS IS: 0.01664; KL IS : 522.61322\n",
      "EPOACH 521: TRAIN LOSS: 0.0160; VAL LOSS IS: 0.01615; KL IS : 519.97327\n",
      "EPOACH 531: TRAIN LOSS: 0.0165; VAL LOSS IS: 0.01657; KL IS : 521.83636\n",
      "EPOACH 541: TRAIN LOSS: 0.0159; VAL LOSS IS: 0.01615; KL IS : 522.98999\n",
      "EPOACH 551: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01594; KL IS : 521.06238\n",
      "EPOACH 561: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01577; KL IS : 521.94220\n",
      "EPOACH 571: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01593; KL IS : 522.74774\n",
      "EPOACH 581: TRAIN LOSS: 0.0162; VAL LOSS IS: 0.01644; KL IS : 523.37250\n",
      "EPOACH 591: TRAIN LOSS: 0.0159; VAL LOSS IS: 0.01600; KL IS : 524.63330\n",
      "EPOACH 601: TRAIN LOSS: 0.0156; VAL LOSS IS: 0.01589; KL IS : 526.10162\n",
      "EPOACH 611: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01607; KL IS : 525.51501\n",
      "EPOACH 621: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01586; KL IS : 524.66479\n",
      "EPOACH 631: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01583; KL IS : 524.16309\n",
      "EPOACH 641: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01599; KL IS : 525.52710\n",
      "EPOACH 651: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01591; KL IS : 521.87775\n",
      "EPOACH 661: TRAIN LOSS: 0.0166; VAL LOSS IS: 0.01671; KL IS : 528.22034\n",
      "EPOACH 671: TRAIN LOSS: 0.0173; VAL LOSS IS: 0.01757; KL IS : 524.90863\n",
      "EPOACH 681: TRAIN LOSS: 0.0156; VAL LOSS IS: 0.01590; KL IS : 527.84149\n",
      "EPOACH 691: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01589; KL IS : 527.29706\n",
      "EPOACH 701: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01592; KL IS : 525.96631\n",
      "EPOACH 711: TRAIN LOSS: 0.0171; VAL LOSS IS: 0.01729; KL IS : 528.46039\n",
      "EPOACH 721: TRAIN LOSS: 0.0156; VAL LOSS IS: 0.01590; KL IS : 528.04517\n",
      "EPOACH 731: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01592; KL IS : 522.95392\n",
      "EPOACH 741: TRAIN LOSS: 0.0162; VAL LOSS IS: 0.01649; KL IS : 525.71356\n",
      "EPOACH 751: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01611; KL IS : 520.94513\n",
      "EPOACH 761: TRAIN LOSS: 0.0160; VAL LOSS IS: 0.01603; KL IS : 526.37109\n",
      "EPOACH 771: TRAIN LOSS: 0.0160; VAL LOSS IS: 0.01623; KL IS : 530.25543\n",
      "EPOACH 781: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01596; KL IS : 528.41437\n",
      "EPOACH 791: TRAIN LOSS: 0.0160; VAL LOSS IS: 0.01631; KL IS : 530.35870\n",
      "EPOACH 801: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01581; KL IS : 526.98724\n",
      "EPOACH 811: TRAIN LOSS: 0.0162; VAL LOSS IS: 0.01644; KL IS : 531.69482\n",
      "EPOACH 821: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01606; KL IS : 528.09705\n",
      "EPOACH 831: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01583; KL IS : 526.60358\n",
      "EPOACH 841: TRAIN LOSS: 0.0155; VAL LOSS IS: 0.01569; KL IS : 528.68213\n",
      "EPOACH 851: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01607; KL IS : 529.96326\n",
      "EPOACH 861: TRAIN LOSS: 0.0171; VAL LOSS IS: 0.01695; KL IS : 531.28674\n",
      "EPOACH 871: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01597; KL IS : 528.80676\n",
      "EPOACH 881: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01588; KL IS : 527.31104\n",
      "EPOACH 891: TRAIN LOSS: 0.0158; VAL LOSS IS: 0.01618; KL IS : 531.90009\n",
      "EPOACH 901: TRAIN LOSS: 0.0155; VAL LOSS IS: 0.01577; KL IS : 529.65851\n",
      "EPOACH 911: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01588; KL IS : 530.92236\n",
      "EPOACH 921: TRAIN LOSS: 0.0166; VAL LOSS IS: 0.01674; KL IS : 529.64087\n",
      "EPOACH 931: TRAIN LOSS: 0.0154; VAL LOSS IS: 0.01574; KL IS : 528.64447\n",
      "EPOACH 941: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01609; KL IS : 532.27783\n",
      "EPOACH 951: TRAIN LOSS: 0.0161; VAL LOSS IS: 0.01622; KL IS : 526.55762\n",
      "EPOACH 961: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01605; KL IS : 521.14569\n",
      "EPOACH 971: TRAIN LOSS: 0.0159; VAL LOSS IS: 0.01631; KL IS : 522.13251\n",
      "EPOACH 981: TRAIN LOSS: 0.0160; VAL LOSS IS: 0.01631; KL IS : 520.75745\n",
      "EPOACH 991: TRAIN LOSS: 0.0157; VAL LOSS IS: 0.01600; KL IS : 519.36328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP_VD(\n",
       "  (log_alpha): ParameterList(\n",
       "      (0): Parameter containing: [torch.FloatTensor of size 8]\n",
       "      (1): Parameter containing: [torch.FloatTensor of size 50]\n",
       "      (2): Parameter containing: [torch.FloatTensor of size 50]\n",
       "      (3): Parameter containing: [torch.FloatTensor of size 20]\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (2): Linear(in_features=50, out_features=20, bias=True)\n",
       "    (3): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP_VD(layers)\n",
    "training_VD(mlp, x, y, x_test, y_test,learning_rate, batch_size, num_epoch, tolerance, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menas.shape = (100, 4015)\n"
     ]
    }
   ],
   "source": [
    "# MC sample \n",
    "K_test = 100 # sample 100 times \n",
    "mlp.train() \n",
    "test = mlp(x_test)\n",
    "MC_samples = [mlp(x_test) for _ in range(K_test)]  \n",
    "# calculate the means \n",
    "means = torch.stack([tup for tup in MC_samples]).view(K_test, x_test.shape[0]).cpu().data.numpy()\n",
    "print(\"menas.shape =\" ,means.shape)\n",
    "mean = np.mean(means, 0)\n",
    "variational_uncertainty = np.std(means, 0)\n",
    "# dropout_probability\n",
    "#dropout_probability = np.array([torch.sigmoid(module.p_logit).cpu().data.numpy()[0] for module in mlp_cd_var.modules() if hasattr(module, 'p_logit')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the area between two lines is =  0.2797820672478207\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hit probability\n",
    "def hit_probability(y_test, mean, variational_uncertainty, confidence):\n",
    "    confidence_lower, confidence_higher = scipy.stats.norm.interval(confidence , mean, variational_uncertainty)\n",
    "#     print(\"confidence_higher = \", confidence_higher) \n",
    "#     print(\"y_test = \", y_test)\n",
    "#     print(\"confidence_lower =\",      confidence_lower)\n",
    "    return np.sum([1 if y_test[i] <= confidence_higher[i] and y_test[i] >= confidence_lower[i] else 0 for i in range(len(y_test))]) / len(y_test)\n",
    "\n",
    "\n",
    "confidence = np.linspace(0, 1.0, 21, endpoint = True)\n",
    "plt.plot(confidence, confidence, '-*')\n",
    "\n",
    "hit_ratio = [hit_probability(y_test, mean, variational_uncertainty, c) for c in confidence]\n",
    "plt.plot(confidence, hit_ratio)\n",
    "\n",
    "area1 = trapz(confidence, dx = 0.05)\n",
    "area2 = trapz(hit_ratio, dx = 0.05)\n",
    "print(\"the area between two lines is = \", area1 - area2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
